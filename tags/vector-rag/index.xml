<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>vector-rag on Jennifer Reif</title><link>https://jmhreif.com/tags/vector-rag/</link><description>Recent content in vector-rag on Jennifer Reif</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 30 Apr 2025 09:00:00 -0600</lastBuildDate><atom:link href="https://jmhreif.com/tags/vector-rag/index.xml" rel="self" type="application/rss+xml"/><item><title>Intro to RAG: Foundations of Retrieval Augmented Generation, part 2</title><link>https://jmhreif.com/blog/2025/intro-to-rag-pt2/</link><pubDate>Wed, 30 Apr 2025 09:00:00 -0600</pubDate><guid>https://jmhreif.com/blog/2025/intro-to-rag-pt2/</guid><description>Photo credit In the last post, we discussed the basics of Retrieval Augmented Generation (RAG) and how it enhances the capabilities of Large Language Models (LLMs) by integrating them with external knowledge sources. We also introduced the concept of vector embeddings and their role in semantic search.
In this post, we’ll dive deeper into the different layers of RAG, including vector RAG, graph RAG, and agents. We’ll explore how these layers can be combined to create more powerful and effective AI systems.</description></item></channel></rss>