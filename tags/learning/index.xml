<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>learning on Jennifer Reif</title><link>https://jmhreif.com/tags/learning/</link><description>Recent content in learning on Jennifer Reif</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 22 Apr 2025 09:00:00 -0600</lastBuildDate><atom:link href="https://jmhreif.com/tags/learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Intro to RAG: Foundations of Retrieval Augmented Generation, part 1</title><link>https://jmhreif.com/blog/2025/intro-to-rag-foundations/</link><pubDate>Tue, 22 Apr 2025 09:00:00 -0600</pubDate><guid>https://jmhreif.com/blog/2025/intro-to-rag-foundations/</guid><description>Photo credit Retrieval Augmented Generation (RAG) may sound complex, but it accurately represents the process of the system. RAG is a method that enhances the capabilities of Large Language Models (LLMs) by integrating them with external knowledge sources.
Each term represents a piece of the puzzle:
Retrieval - data retrieved from some external source outside the LLM (most often a database, but can include files, webpages, etc)
Augmented - &amp;#34;augmenting&amp;#34; (or adding to) an LLM’s training data.</description></item><item><title>GenAI blood, sweat, and tears: Loading data to Pinecone</title><link>https://jmhreif.com/blog/2025/vector-graph-rag/</link><pubDate>Wed, 09 Apr 2025 09:00:00 -0600</pubDate><guid>https://jmhreif.com/blog/2025/vector-graph-rag/</guid><description>Photo credit As someone who is pretty familiar with relational and graph databases, I wanted to dig a little deeper into vector databases and understand the strengths and quirks they bring to the database table. I put together a conference abstract on vector RAG versus GraphRAG which got picked up, so I went to work building a demo and learning all I could.
I pivoted a few times along the way, but ended up with a Spring AI application that connects to both Pinecone (vector database) and Neo4j (graph database).</description></item><item><title>What I Learned Going from Intel to Apple Silicon</title><link>https://jmhreif.com/blog/2021/docker-intel-to-m1/</link><pubDate>Thu, 04 Nov 2021 09:00:00 -0600</pubDate><guid>https://jmhreif.com/blog/2021/docker-intel-to-m1/</guid><description>Photo credit Earlier this year, I received a new work laptop with the recent Apple silicon chip (versus Intel chip). The new M1 has been critically examined with avid enthusiasm and criticism, so I wasn’t sure what my outcome would be.
Thus far, it’s been positive, and I really haven’t noticed too much disruption in my work. Things have been faster, but I’m not sure whether to entirely attribute that new chip itself, or just the new machine, in general.</description></item><item><title>In the Language Wars, Java Holds Its Own</title><link>https://jmhreif.com/blog/2019/java-holds-its-own/</link><pubDate>Mon, 18 Nov 2019 12:00:00 -0600</pubDate><guid>https://jmhreif.com/blog/2019/java-holds-its-own/</guid><description>We all pick our favorites and downplay other options (colors, cars, sports team, etc.). Programming language choice is not exempt. Whether it’s the one we are most comfortable with or the one that got us a job, we cling to that choice.
Today, we will focus on Java. There are perfectly valid complaints and praises for this language, and we will cover them. As always, these are my experiences, so others may see things differently.</description></item></channel></rss>