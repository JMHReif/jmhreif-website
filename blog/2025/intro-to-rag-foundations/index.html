<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Intro to RAG: Foundations of Retrieval Augmented Generation, part 1 | Jennifer Reif</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Photo credit Retrieval Augmented Generation (RAG) may sound complex, but it accurately represents the process of the system. RAG is a method that enhances the capabilities of Large Language Models (LLMs) by integrating them with external knowledge sources.
Each term represents a piece of the puzzle:
Retrieval - data retrieved from some external source outside the LLM (most often a database, but can include files, webpages, etc)
Augmented - &#34;augmenting&#34; (or adding to) an LLM’s training data."><meta name=generator content="Hugo 0.119.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><link rel=canonical href=https://jmhreif.com/blog/2025/intro-to-rag-foundations/><meta property="og:title" content="Intro to RAG: Foundations of Retrieval Augmented Generation, part 1"><meta property="og:description" content="Photo credit Retrieval Augmented Generation (RAG) may sound complex, but it accurately represents the process of the system. RAG is a method that enhances the capabilities of Large Language Models (LLMs) by integrating them with external knowledge sources.
Each term represents a piece of the puzzle:
Retrieval - data retrieved from some external source outside the LLM (most often a database, but can include files, webpages, etc)
Augmented - &#34;augmenting&#34; (or adding to) an LLM’s training data."><meta property="og:type" content="article"><meta property="og:url" content="https://jmhreif.com/blog/2025/intro-to-rag-foundations/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-04-22T09:00:00-06:00"><meta property="article:modified_time" content="2025-04-22T09:00:00-06:00"><meta itemprop=name content="Intro to RAG: Foundations of Retrieval Augmented Generation, part 1"><meta itemprop=description content="Photo credit Retrieval Augmented Generation (RAG) may sound complex, but it accurately represents the process of the system. RAG is a method that enhances the capabilities of Large Language Models (LLMs) by integrating them with external knowledge sources.
Each term represents a piece of the puzzle:
Retrieval - data retrieved from some external source outside the LLM (most often a database, but can include files, webpages, etc)
Augmented - &#34;augmenting&#34; (or adding to) an LLM’s training data."><meta itemprop=datePublished content="2025-04-22T09:00:00-06:00"><meta itemprop=dateModified content="2025-04-22T09:00:00-06:00"><meta itemprop=wordCount content="1758"><meta itemprop=keywords content="rag,vector,llm,learning,genai,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Intro to RAG: Foundations of Retrieval Augmented Generation, part 1"><meta name=twitter:description content="Photo credit Retrieval Augmented Generation (RAG) may sound complex, but it accurately represents the process of the system. RAG is a method that enhances the capabilities of Large Language Models (LLMs) by integrating them with external knowledge sources.
Each term represents a piece of the puzzle:
Retrieval - data retrieved from some external source outside the LLM (most often a database, but can include files, webpages, etc)
Augmented - &#34;augmenting&#34; (or adding to) an LLM’s training data."></head><body class="ma0 avenir bg-near-white"><header class="cover bg-top" style=background-image:url(https://jmhreif.com/img/intro-to-rag/unsplash-layers-rag.jpg)><div class=bg-black-60><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Jennifer Reif</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/ title="Home page">Home</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/about/ title="About page">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/blog/ title="Blog page">Blog</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/abstracts/ title="Abstracts page">Abstracts</a></li></ul><div class=ananke-socials><a href=https://twitter.com/JMHReif target=_blank rel=noopener class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" aria-label="follow on Twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href=https://github.com/JMHReif target=_blank rel=noopener class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href=https://www.linkedin.com/in/jmhreif/ target=_blank rel=noopener class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href=https://jmhreif.com/index.xml target=_blank rel=noopener class="rss ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="RSS link" aria-label="follow on RSS——Opens in a new window"><span class=icon><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"/><path id="scale" d="M4 4.44v2.83c7.03.0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9.0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href=https://jmhreif.podbean.com/ target=_blank rel=noopener class="podcast ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="podcast link" aria-label="follow on podcast——Opens in a new window"><span class=icon><svg width="1200pt" height="1200pt" viewBox="0 0 1200 1200" xmlns="http://www.w3.org/2000/svg"><path d="m6e2 62.484c-296.48.0-537.52 241.03-537.52 537.52s240.98 537.52 537.52 537.52c296.48.0 537.52-240.98 537.52-537.52.0-296.48-241.03-537.52-537.52-537.52zm-155.02 305.02c0-85.5 69.516-155.02 155.02-155.02S755.02 282 755.02 367.504v234.98c0 85.5-69.516 155.02-155.02 155.02s-155.02-69.516-155.02-155.02zm192.52 516.52v66c0 20.484-17.016 37.5-37.5 37.5s-37.5-17.016-37.5-37.5v-66C408 865.508 288 734.024 288 574.504c0-20.484 17.016-37.5 37.5-37.5s37.5 17.016 37.5 37.5c0 130.5 106.5 237 237 237s237-106.5 237-237c0-20.484 17.016-37.5 37.5-37.5s37.5 17.016 37.5 37.5c0 159.52-120 291-274.5 309.52z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></nav><div class="tc-l pv6 ph3 ph4-ns"></div></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked ttu">Blog</aside><div id=sharing class="mt3 ananke-socials"><a href="https://twitter.com/intent/tweet?url=https://jmhreif.com/blog/2025/intro-to-rag-foundations/&amp;text=Intro%20to%20RAG:%20Foundations%20of%20Retrieval%20Augmented%20Generation,%20part%201" class="ananke-social-link twitter no-underline" aria-label="share on Twitter"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://jmhreif.com/blog/2025/intro-to-rag-foundations/&amp;title=Intro%20to%20RAG:%20Foundations%20of%20Retrieval%20Augmented%20Generation,%20part%201" class="ananke-social-link linkedin no-underline" aria-label="share on LinkedIn"><span class=icon><svg style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div><h1 class="f1 athelas mt3 mb1">Intro to RAG: Foundations of Retrieval Augmented Generation, part 1</h1><time class="f6 mv4 dib tracked" datetime=2025-04-22T09:00:00-06:00>April 22, 2025</time>
<span class="f6 mv4 dib tracked">- 9 minutes read</span>
<span class="f6 mv4 dib tracked">- 1758 words</span></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><div class=paragraph><small><i><a href="https://unsplash.com/photos/orange-pink-and-teal-illustration-Tk0B3Dfkf_4?utm_content=creditShareLink&amp;utm_medium=referral&amp;utm_source=unsplash">Photo credit</a></i></small></div><div class=paragraph><p>Retrieval Augmented Generation (RAG) may sound complex, but it accurately represents the process of the system. RAG is a method that enhances the capabilities of Large Language Models (LLMs) by integrating them with external knowledge sources.</p></div><div class=paragraph><p>Each term represents a piece of the puzzle:</p></div><div class=ulist><ul><li><p>Retrieval - data retrieved from some external source outside the LLM (most often a database, but can include files, webpages, etc)</p></li><li><p>Augmented - "augmenting" (or adding to) an LLM’s training data. This could include recent or private information that it did not have access to during its training period. Most often, this is done by adding the data to the prompt (or input) to the LLM.</p></li><li><p>Generation - this is where LLMs are exceptional. They generate a response (text, image, video, etc) that is similar to the data being provided in the input. This is generated from probabilities, so cannot guarantee 100% consistency.</p></li></ul></div><div class=paragraph><p>Instead of relying solely on the model’s internal training data, RAG retrieves relevant information from databases or document collections to ground its responses in factual and up-to-date content. This approach not only improves the accuracy and reliability of the generated outputs but also allows the system to adapt to specific contexts or domains, making it a powerful tool for many personal and professional applications.</p></div><div class=paragraph><p>In this post, we will define each component and how it works together in a RAG system.</p></div><div class=sect1><h2 id=_why_rag>Why RAG?</h2><div class=sectionbody><div class=paragraph><p>Retrieval augmented generation (RAG) solves a few different problems in the technical space.</p></div><div class="olist arabic"><ol class=arabic><li><p>It provides a way to dynamically add data/information to (augment) an LLM’s knowledge, improving relevance and accuracy in answers.</p></li><li><p>It provides searchable access for all types of data storage - database types, text, images, audio, video, webpages, etc.</p></li><li><p>It allows technical experts to guide or limit the AI with defined tools, high-quality data, rules, business logic, and more. This increases accuracy and reduces risk of the system.</p></li></ol></div></div></div><div class=sect1><h2 id=_large_language_models_llms>Large Language Models (LLMs)</h2><div class=sectionbody><div class=imageblock><div class=content><img src=/img/intro-to-rag/areas-of-ai.jpg alt="Fields within Artificial Intelligence" width=400></div></div><div class=paragraph><p>Part of the Generative AI field, Large Language Models (LLMs) are advanced AI systems designed to generate content by predicting the probabilities of sequences within their input data. They excel at understanding context and producing coherent outputs, making them versatile tools for a wide range of applications.</p></div><div class=paragraph><p>However, they also have limitations. LLMs generate responses on probabilities, leaving room for inconsistency or uncertainty, especially when there are multiple potential answers and no high probability for any of the options.</p></div><div class=paragraph><p>These models can process various types of input/output (modalities), but their performance is constrained by the size of their context windows - the amount of information they can consider at once - and the quality of the prompts provided.</p></div><hr><div class=paragraph><p><strong>Note:</strong> Each Large Language Model (LLM) is trained slightly differently to prioritize certain probabilities over others to optimize for certain goals. This is why every LLM may produce different outputs for the same input and why you should evaluate different models and research which ones might be pre-optimized for your needs.</p></div><hr><div class=paragraph><p>There are a variety of reasons that Large Language Models tend to hallucinate (or produce inaccurate, non-sensical answers). A few of those include the following:</p></div><div class=ulist><ul><li><p>Searching for answers related to recent or private data that the LLM has not been trained on or has access to.</p></li><li><p>The prompt (input) traverses gaps or limits in the LLMs "knowledge" pathways, getting stuck or not enough next thoughts to generate.</p></li><li><p>The LLM does not have enough context (background information) to guide its answer towards a specific path (too much uncertainty in the input’s meaning).</p></li></ul></div><div class=paragraph><p>How do we improve these weaknesses by providing context to the LLM?</p></div></div></div><div class=sect1><h2 id=_vector_embeddings>Vector embeddings</h2><div class=sectionbody><div class=paragraph><p>A <a href=https://www.mathsisfun.com/algebra/vectors.html target=_blank rel=noopener>vector is a mathematical concept</a> representing a line that has a size (magnitude) and direction. This numeric representation allows us to make calculations and comparisons to explain forces in physics. In the real world, we use vectors for a couple of relatable use cases.</p></div><div class=paragraph><p><strong>1. Airplane flight paths in 3-dimensional space.</strong> If you think about a flight path, there may be landmark features along it such as buildings, rivers, and airspaces (military, city, or airport restriction areas). Planes also need to take external factors into account, such as wind and storms. Not only do they need a representation of where they are in the air, they need to be able to calculate changes to that flight path due to obstacles or real-time airspace restrictions. They do this by creating a numeric representation (vector) of that path and calculating with other vectors for winds, weather areas, and more.</p></div><div class=paragraph><p><span class=image><img src=/img/intro-to-rag/vector-airplane.png alt="Airplane vectors" width=400></span></p></div><div class=paragraph><p><strong>2. Trajectories of rockets in multi-dimensional space.</strong> Similar to the airplane example, but outer space deals in multi-dimensional space and more lethal "features" (obstacles) along a path like black holes, asteroid belts, and planets. Scientists would need to calculate vector routes to avoid passing through planets and avoid gravitational pulls from celestial bodies.</p></div><div class=paragraph><p>We represent the paths by creating numeric representations based upon key, defined features that characterize the path (vector). Then, we can use those paths to make calculations and precise adjustments based on external factors.</p></div><div class=sect2><h3 id=_vectors_applied_to_words>Vectors applied to words</h3><div class=paragraph><p>In 2013, <a href=https://code.google.com/archive/p/word2vec/ target=_blank rel=noopener>Google applied this mathmatical concept to words</a> (word2vec), creating numeric representations of words based on how they functioned within the language and defining characteristics. Word embeddings map words into a continuous vector space where semantically similar words are closer together. For instance, the words "king" and "queen" might have embeddings that are close in this space, reflecting their related meanings around power, leadership, luxurious living, and wealth.</p></div><div class=imageblock><div class=content><img src=/img/intro-to-rag/vector-words.png alt="Vectors with words"></div></div><div class=paragraph><p>This ability allowed humans to represent words for comparing similarity of words or understanding new words from proximity to known words. Broader searches and synonym lists based on "semantic" meaning could be factored into the calculations, which are foundational for many natural language processing tasks.</p></div></div><div class=sect2><h3 id=_vectors_applied_to_data>Vectors applied to data</h3><div class=paragraph><p>We took this one step further in the last few years to apply this to any type of data (text, image, video, audio, etc). Vector embeddings are numerical representations of data that capture semantic meaning in a way that makes it easier to compare and analyze.</p></div><div class=imageblock><div class=content><img src=/img/intro-to-rag/data-embeddings.png alt="Data embeddings" width=500></div></div><div class=paragraph><p>When combined with generative AI (GenAI), embeddings enable semantic searches, which go beyond simple keyword matching. Unlike lexical searches that rely on exact word matches, semantic searches use embeddings to understand the meaning behind the query and retrieve results that are contextually relevant. This makes them particularly powerful for applications like document retrieval, where understanding the intent and context of a query is crucial for delivering accurate and meaningful results.</p></div><div class=paragraph><p>There is a common saying that you can’t <a href=https://en.wikipedia.org/wiki/Apples_and_oranges target=_blank rel=noopener>compare apples and oranges</a> (because they have two different sets of characteristics). However, with a numeric representation, we actually can now compare them because we have a common format to represent all sorts of objects and data.</p></div><div class=paragraph><p>Also, a recent article I read compared vectors to a "fingerprint" of the data. Just as a fingerprint is unique to each individual, the vector representation of a piece of data is unique to that specific data point. This uniqueness allows for precise identification and retrieval of information, even in large datasets.</p></div><hr><div class=paragraph><p><strong>Note:</strong> Since each LLM is trained slightly differently, the vector embeddings may be different for each model. This means that the same piece of data may have slightly different vector representations with different models (though both will be close together in the vector space). This is important to consider when using multiple LLMs or comparing results across models.</p></div><hr><div class=paragraph><p>Here enter the need and purpose of <a href=https://frankzliu.com/blog/a-gentle-introduction-to-vector-databases target=_blank rel=noopener>vector databases</a>, which are optimized to store and search these vector representations. But how do vector databases efficiently search vast amounts of these numbers (think every word in every language or millions of text documents)?</p></div></div><div class=sect2><h3 id=_similarity_search>Similarity search</h3><div class=paragraph><p>Similarity search involves finding data records that are most similar to a given query. This is often achieved using techniques like k-Nearest Neighbors (k-NN) or approximate methods like k-ANN for efficiency, where <code>k</code> represents the number of most similar results you want returned (i.e. 7, 42, 100).</p></div><div class=paragraph><p>This might seem overly complex, but let’s look at an example to understand the power of these types of searches. Let’s think about a library.</p></div><div class=imageblock><div class=content><img src=/img/intro-to-rag/library-classification-search.jpg alt="Library classification and search" width=500></div></div><div class=paragraph><p>In a library today, searching for a new book to read would require picking from the nested category structure of organizing books (e.g. fiction/non-fiction → genre → author → title). If I wanted to read a fantasy novel, my current strategy would be to walk to the fiction area, find the fantasy section, and start pulling books off the shelf to see what sparked my interest. Another alternative would be to do a computer search for keywords and hope that the book is tagged with the topics I’m interested in.</p></div><div class=paragraph><p>Vectors would allow us to <a href=https://towardsdatascience.com/explaining-vector-databases-in-3-levels-of-difficulty-fc392e48ab78/ target=_blank rel=noopener>search for books based on semantics</a>, finding similarities for specific features that are baked into the vector embedding and returning results in the nearby vector space as our search query.</p></div><div class=imageblock><div class=content><img src=/img/intro-to-rag/vector-similarity-search.png alt="Vector similarity search" width=200></div></div><div class=paragraph><p>To measure similarity, cosine similarity and euclidean distance are two of the most common metrics used, though there are others as well. Cosine similarity measures the distance between the angle of the vectors. Remember, vectors are lines with a length and direction, so cosine measures the distance between the two lines in degrees. Euclidean distance is the shortest distance from point-to-point ("as the crow flies" between vector points).</p></div><div class=imageblock><div class=content><img src=/img/intro-to-rag/cosine-vs-euclidean-similarity.png alt="Cosine similarity vs Euclidean distance measure" width=500></div></div><div class=paragraph><p>In our library example, we could search for specific features like "dragons and magic" or "based in St. Louis, USA". These criteria are much narrower and much more likely to find a smaller result set that is more relevant to what the user is searching for.</p></div><hr><div class=paragraph><p><strong>Note:</strong> Vector embeddings differ for each model, and each vector store also optimizes vector similarity search differently. So even the same data and embeddings stored in different vector stores may produce different results from a similarity search.</p></div><hr></div></div></div><div class=sect1><h2 id=_wrapping_up>Wrapping up!</h2><div class=sectionbody><div class=paragraph><p>In this blog post, we explored a few introductory concepts around Retrieval Augmented Generation (RAG), why it exists and the problems it solves. We also covered some starting GenAI concepts on Large Language Models (LLMs), vectors and embeddings, and vector similarity search. These pieces build the foundations of more complex AI systems and how RAG enhances the capabilities of LLMs by integrating them with external knowledge sources.</p></div><div class=paragraph><p>In another post, we will explore the different layers of RAG, including vector RAG, graph RAG, and agents.</p></div><div class=paragraph><p>Whether you’re a developer, data scientist, or simply someone interested in the future of AI, understanding AI technologies and how they operate will empower you to make better decisions on how to use them.</p></div><div class=paragraph><p>Happy coding!</p></div></div></div><div class=sect1><h2 id=_resources>Resources</h2><div class=sectionbody><div class=ulist><ul><li><p>Tutorial: <a href=https://www.mathsisfun.com/algebra/vectors.html target=_blank rel=noopener>Vectors - Math is Fun</a></p></li><li><p>Project: <a href=https://code.google.com/archive/p/word2vec/ target=_blank rel=noopener>word2vec - Google</a></p></li><li><p>Blog post: <a href=https://towardsdatascience.com/explaining-vector-databases-in-3-levels-of-difficulty-fc392e48ab78 target=_blank rel=noopener>Explaining Vector Databases in 3 Levels of Difficulty - Towards Data Science</a></p></li></ul></div></div></div><ul class=pa0><li class="list di"><a href=/tags/rag/ class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">rag</a></li><li class="list di"><a href=/tags/vector/ class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">vector</a></li><li class="list di"><a href=/tags/llm/ class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">llm</a></li><li class="list di"><a href=/tags/learning/ class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">learning</a></li><li class="list di"><a href=/tags/genai/ class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">genai</a></li></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"><div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links"><p class="f5 b mb3">Related</p><ul class="pa0 list"><li class=mb2><a href=/blog/2025/vector-graph-rag/>GenAI blood, sweat, and tears: Loading data to Pinecone</a></li><li class=mb2><a href=/blog/2024/spring-ai-update-10/>Spring AI Upgrade: Changes for Applications from 0.8 to 1.0</a></li><li class=mb2><a href=/blog/2024/spring-ai-starter-kit/>GenAI Starter Kit: Everything You Need to Build an Application with Spring AI in Java</a></li><li class=mb2><a href=/blog/2024/spring-ai-app/>Spring AI: How to Write GenAI Applications with Java</a></li><li class=mb2><a href=/blog/2024/rag-demo-retrieval/>Implementing RAG: How to write a graph retrieval query in LangChain</a></li><li class=mb2><a href=/blog/2021/docker-intel-to-m1/>What I Learned Going from Intel to Apple Silicon</a></li><li class=mb2><a href=/blog/2019/java-holds-its-own/>In the Language Wars, Java Holds Its Own</a></li></ul></div></aside></article></main><footer class="bg-dark-green bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://jmhreif.com/>&copy; Jennifer Reif 2025</a><div><div class=ananke-socials><a href=https://twitter.com/JMHReif target=_blank rel=noopener class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" aria-label="follow on Twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href=https://github.com/JMHReif target=_blank rel=noopener class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href=https://www.linkedin.com/in/jmhreif/ target=_blank rel=noopener class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href=https://jmhreif.com/index.xml target=_blank rel=noopener class="rss ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="RSS link" aria-label="follow on RSS——Opens in a new window"><span class=icon><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"/><path id="scale" d="M4 4.44v2.83c7.03.0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9.0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href=https://jmhreif.podbean.com/ target=_blank rel=noopener class="podcast ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="podcast link" aria-label="follow on podcast——Opens in a new window"><span class=icon><svg width="1200pt" height="1200pt" viewBox="0 0 1200 1200" xmlns="http://www.w3.org/2000/svg"><path d="m6e2 62.484c-296.48.0-537.52 241.03-537.52 537.52s240.98 537.52 537.52 537.52c296.48.0 537.52-240.98 537.52-537.52.0-296.48-241.03-537.52-537.52-537.52zm-155.02 305.02c0-85.5 69.516-155.02 155.02-155.02S755.02 282 755.02 367.504v234.98c0 85.5-69.516 155.02-155.02 155.02s-155.02-69.516-155.02-155.02zm192.52 516.52v66c0 20.484-17.016 37.5-37.5 37.5s-37.5-17.016-37.5-37.5v-66C408 865.508 288 734.024 288 574.504c0-20.484 17.016-37.5 37.5-37.5s37.5 17.016 37.5 37.5c0 130.5 106.5 237 237 237s237-106.5 237-237c0-20.484 17.016-37.5 37.5-37.5s37.5 17.016 37.5 37.5c0 159.52-120 291-274.5 309.52z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></footer></body></html>